<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning">
  <meta property="og:title" content="SPEED-RL"/>
  <meta property="og:description" content="SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning"/>
  <meta property="og:image" content="static/images/proj_fig.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="SPEED-RL">
  <meta name="twitter:description" content="SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning">
  <meta name="twitter:image" content="static/images/proj_fig.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="KV Cache">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning</title>

  <link rel="icon" type="image/png" href="static/images/SpeculativeRejection.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body>

  <!-- Top hero section -->
  <!-- <section class="hero"> -->
  <div>
    <div class="hero-body" style="padding: 1rem; margin-top:4rem">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title" style="display: inline;">
              Training Language Models to Reason Efficiently
            </h1>
            <br><br>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://daman1209arora.github.io"
                   target="_blank"
                   rel="noopener noreferrer">Daman Arora</a>,
              </span>
              <span class="author-block">
                <a href="https://azanette.com/"
                   target="_blank"
                   rel="noopener noreferrer">Andrea Zanette</a>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="affliation">
                <small>Carnegie Mellon University</small>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
  <!-- </section> -->

  <!-- Intro and Buttons Section with bottom margin -->
  <section style="margin-bottom: 2rem;">
    <div class="container">
      <!-- Intro placeholder -->
      <div class="intro">
        <p></p>
      </div>

      <!-- Buttons section with extra top margin for spacing -->
      <div class="buttons-section">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <span class="link-block">
              <a href="https://arxiv.org/abs/2502.04463"
                 target="_blank"
                 rel="noopener noreferrer"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>
            <span class="link-block" style="margin-left: 1rem;">
              <a href="https://github.com/Zanette-Labs/efficient-reasoning"
                 target="_blank"
                 rel="noopener noreferrer"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
    <div style="display: block; text-align: center; padding-top: 1rem">
      <p>
        <strong>TL;DR</strong> We post-train reasoning models with reinforcement learning to reduce token usage while preserving accuracy.
      </p>
    </div>
  </section>

  <!-- Paper abstract (gray area) -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- Extra margin above the Introduction heading -->
           
          <h2 class="title is-3" style="text-align: center; margin-top: 1rem;">
            Introduction
          </h2>
          <div class="content has-text-justified">
            <p>
Training large language models to perform advanced reasoning with reinforcement learning has significantly advanced their problem-solving abilities. 
However, their reliance on long chain-of-thoughts commands a high inference cost, posing challenges for efficient deployment. 
<!-- To address this, we propose to post-train models to use the least possible amount of tokens to reach the correct solution. -->
To address this, we propose post-training reasoning models with reinforcement learning using an objective function that favors correct responses with concise chain-of-thoughts.
            </p>

          <div class="figure" style="text-align: center;">
            <!-- <img src="static/images/normalized_averages copy-1.png" -->
            <img src="static/images/output.png" 
                 alt="Reasoning Efficiently System"
                 style="margin: auto; display: block; width: 80%"  />
            <div style="display: block; width: 65%; text-align: center; margin: 0 auto;">
              <p style="font-size: 0.8em; margin-top: 0.5rem">
                <strong>Figure 1:</strong> Our models achieve comparable accuracy to the Full Reasoning model (DeepSeek-R1-Distill-Qwen-7B) while significantly reducing token consumption during inference. Results are normalized across the GSM8K, MATH500, and AIME2024 datasets; Instruct refers to the Qwen2.5-Math-7B-Instruct model. Token usage and accuracy are normalized relative to the Full Reasoning model. 
              </p>
            </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Second gray-area section (simpler 'section' instead of hero) -->
  <section class="section is-light" style="padding-top: 2rem; padding-bottom: 2rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <!-- Extra margin above the Learning heading -->
<h2 class="title is-3" 
    style="text-align: center; margin-top: 3rem; display: flex; align-items: center; justify-content: center;">
  <!-- Logo on the left -->
  <img src="static/images/Fast.png" 
       alt="Logo" 
       style="height: 40px; margin-right: 10px;" />
  <!-- Heading text -->
  Learning When to Stop Thinking
</h2>
          <div class="content has-text-justified">
            <p style="text-align: justify;">

            </p>
            <p style="text-align: justify;">
              We evaluate our post-training procedure on three common benchmarks, ordered by increasing level of difficulty:
            </p>
            <ul style="list-style-type: disc; padding-left: 1rem; text-align: justify;">
              <li><strong>GSM8K</strong>, a dataset containing grade-school-level math problems;</li>
              <li><strong>MATH500</strong>, a standard benchmark with problems harder than those in GSM8K;</li>
              <li><strong>AIME2024</strong>, a competition-level dataset of challenging mathematical problems.</li>
            </ul>
            <div class="figure">
              <img src="static/images/result.png"
                   alt="Reasoning Efficiently System"
                   height="400" />
              
              <p style="text-align: center; font-size: 0.8em; margin-top: 0.5rem; margin-bottom: 1rem; display: block; width: 68%; text-align: center; margin: 0 auto;">
                <strong>Figure 2:</strong> Detailed comparison of the number of tokens and accuracy achieved by different models to solve problems in the GSM8K, MATH500, and AIME2024 datasets.
              </p>
            </div>
            <p style="text-align: justify; margin-top: 1rem;">
              <!-- Our post-training procedure generates a <strong>family of reasoning models</strong>, governed by a -->
              <!-- coefficient &alpha;. Starting from a reasoning model such as DeepSeek-R1-Distill-Qwen-7B (labeled "Full Reasoning" in the -->
              <!-- figure), increasing &alpha; boosts generation efficiency—yielding shorter chain-of-thoughts—while largely preserving response accuracy. -->
              Our post-training procedure refines a reasoning model—such as DeepSeek-R1-Distill-Qwen-7B, labeled "Full Reasoning" in the figure—into 
              a more <strong>token-efficient</strong> version. By selecting a scalar coefficient α at the beginning of post-training, users can balance accuracy 
              and token efficiency. A higher α enhances generation efficiency by shortening chain-of-thoughts while largely maintaining response accuracy.
            </p>
            <p style="text-align: justify;">
              Models trained in this way <strong>know when to stop thinking</strong>:
              they recognize when they have found the correct answer and conclude their reasoning efficiently.
              For straightforward problems (e.g., those in GSM8K) these models deliver efficient, direct
              solutions, while for more demanding ones (e.g., those in AIME2024), they invest additional
              computational effort to perform advanced reasoning; see Figure 2 for additional details.
            </p>
          </div>

          <!-- Extra margin above the Training Procedure heading -->
<h2 class="title is-3" 
    style="text-align: center; margin-top: 7rem; display: flex; align-items: center; justify-content: center;">
  <!-- Logo on the left -->
  <img src="static/images/train.png" 
       alt="Logo" 
       style="height: 40px; margin-right: 10px;" />
  <!-- Heading text -->
  Post-Training Procedure
</h2>

          <div id="main-content">
            <div class="container">
              <div class="figure" style="margin-top: 2rem;">
                <img src="static/images/pipeline.drawio-5.png"
                     alt="Training Method Overview"
                     height="400" />
                <p style="text-align: center; font-size: 0.8em; margin-top: 0.5rem; margin-bottom: 1rem;">
                  <strong>Figure 3:</strong> Overview of the training method. The ❌ and ✅ symbols indicate incorrect and correct responses, respectively.
                </p>
              </div>
              <p style="margin-bottom: 3rem ;">
                We use reinforcement learning to post-train models for both accuracy and token efficiency. For each prompt, multiple solutions are sampled and rewarded based on correctness and response length. The shortest correct answers receive the highest rewards, followed by longer correct ones, while incorrect responses receive the lowest rewards. The models are then updated using policy gradients.
              </p>
              <div class="figure" style="margin-top: 4rem; display: block; width: 75%; text-align: center; margin: 0 auto;">
                <img src="static/images/eqn.drawio-4-1.png"
                     alt="Training Method Overview"
                     style="width: 100%; display: block; margin: auto;" />    
                <p style="text-align: center; font-size: 0.8em; margin-top: 0.5rem;">
                  <strong>Figure 4:</strong> Reward function used for training. Incorrect responses receive a reward of 0, while correct responses are assigned higher rewards if they are shorter compared to other correct responses. To compute the self-normalized length penalty of a response \( y \) with respect to a prompt \( x \), the mean \( \texttt{MEAN}(x) \) and variance \( \texttt{VAR}(x) \) of correct response lengths are calculated for each prompt \( x \). Here \( \mathbb I_{correct}(x,y) \) indicates that the response is correct.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- BibTeX citation -->
  <section class="section" id="BibTeX">
    <div class="container has-text-left">
      <!-- If you also want extra space above "BibTeX", do margin-top here as well -->
      <h3 class="title is-5" style="margin-top: 3rem;">
        BibTeX
      </h3>
      <pre><code>@article{arora2025traininglanguagemodelsreason,
  title={Training Language Models to Reason Efficiently}, 
  author={Daman Arora and Andrea Zanette},
  year={2025},
  eprint={2502.04463},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2502.04463},
}
</code></pre>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <!-- Footer content -->
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
